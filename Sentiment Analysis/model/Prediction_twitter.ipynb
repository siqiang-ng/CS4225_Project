{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Prediction_twitter.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eHmXS-cAVhvz"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import codecs\n","import keras\n","import h5py\n","import jax\n","import jax.numpy as jnp #gonna try use jax since its supposedly faster\n","import time\n","import glob\n","import ast\n","from numpy import interp\n","\n","# Keras model, layer and preprocessing libraries\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import LSTM\n","from keras.layers.embeddings import Embedding\n","from keras.layers import Bidirectional\n","from keras.layers import Dropout\n","from keras.models import Sequential\n","from keras.models import model_from_json\n","from keras.models import load_model\n","from keras.preprocessing import sequence\n","\n","# NLTK imports\n","from nltk.tokenize import RegexpTokenizer"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.flush_and_unmount()"],"metadata":{"id":"2P0L5JoMVzfC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount(\"/content/gdrive\")"],"metadata":{"id":"htHRQVrJV0rD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651344776182,"user_tz":-480,"elapsed":5270,"user":{"displayName":"Nigel Chua","userId":"17321780526605667435"}},"outputId":"cb6a5a32-f18f-48c1-e1a3-d8310f52b049"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["import os\n","\n","cwd = os.getcwd()  "],"metadata":{"id":"L8xgTpyWV1yY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Path to the data\n","gloveFile = '/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/glove.6B.100d.txt'\n","\n","word_embedding_dict = {}\n","word_index = {}\n","\n","with codecs.open(gloveFile, 'r', encoding='utf-8') as f:\n","    for line in f:\n","        values = line.split()\n","        # The word that is to be the key\n","        word = values[0]\n","        # Update word_index\n","        word_index[word] = len(word_embedding_dict)\n","        # The vector that represents the word\n","        vector = np.asarray(values[1:], \"float32\")\n","        word_embedding_dict[word] = vector"],"metadata":{"id":"0PAZJV56V3h0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_embedding_matrix = np.zeros((len(word_embedding_dict), 100))\n","\n","i = 0\n","for word in word_embedding_dict.keys():\n","    if i > len(word_embedding_matrix):\n","        break\n","    \n","    word_embedding_vector = word_embedding_dict[word]\n","    if word_embedding_vector is not None:\n","        word_embedding_matrix[i] = word_embedding_vector\n","        i = i + 1"],"metadata":{"id":"FVpgm5PDV-SZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Developing the LSTM Model \n","model = Sequential()\n","# Input_length of 280 is chosen as 280 characters are allowed in twitter\n","model.add(Embedding(len(word_embedding_matrix), 100, weights = [word_embedding_matrix], input_length = 280, trainable = False))\n","model.add(Bidirectional(LSTM(128, dropout = 0.2, recurrent_dropout = 0.2)))\n","model.add(Dense(512, activation = 'relu'))\n","model.add(Dropout(0.50))\n","model.add(Dense(3, activation = 'softmax'))\n","\n","# Adam's optimiser\n","model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","\n","print(model.summary())"],"metadata":{"id":"EVFsWaTOWAdw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650827963759,"user_tz":-480,"elapsed":860,"user":{"displayName":"Kexin Yeo","userId":"10886170617646031738"}},"outputId":"e0713648-feb7-4783-bdde-9fbf72812fc1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 280, 100)          40000000  \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 256)              234496    \n"," nal)                                                            \n","                                                                 \n"," dense_2 (Dense)             (None, 512)               131584    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 512)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 3)                 1539      \n","                                                                 \n","=================================================================\n","Total params: 40,367,619\n","Trainable params: 367,619\n","Non-trainable params: 40,000,000\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["def get_sentiment_sentence(data, word_index):\n","    # Load the RNN model\n","    model.load_weights(\"/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/RNN_Split_Dataset/RNN.h5\")\n","    print(\"Model weight loaded successfully\")\n","    \n","    # Convert keys to lowercase\n","    word_index_lower = {k.lower(): v for k, v in word_index.items()}\n","    \n","    # Preparing the data for prediction\n","    word_list = np.zeros((1, 280), dtype = 'int32')\n","    sentiment_analysis = {}\n","    \n","    for index, indiv_data in data.iterrows():\n","\n","        sentence = indiv_data['text']\n","        #print(indiv_data['text'])\n","        tokenizer = RegexpTokenizer(r'\\w+')\n","        sentence_words = tokenizer.tokenize(str(sentence))\n","           \n","        # Capture all the word into one list for prediction\n","        i = 0\n","        for word in sentence_words:\n","            word_lower = word.lower()\n","            try:\n","                word_list[0][i] = word_index_lower[word_lower]\n","            except Exception as e:\n","                if str(e) == word:\n","                    word_list[0][i] = 0\n","                continue\n","            i = i + 1\n","        \n","        # Predict the score of the tweet \n","        score = model.predict(word_list, batch_size = len(data), verbose = 0)\n","        single_score = np.round(np.argmax(score)/10, decimals=2) # maximum of the array i.e single band\n","        # weighted score of top 3 bands\n","        top_3_index = np.argsort(score)[0][-3:]\n","        top_3_scores = score[0][top_3_index]\n","        top_3_weights = top_3_scores/np.sum(top_3_scores)\n","        single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)\n","        #print(single_score_dot, single_score)\n","        #sentiment_analysis = sentiment_analysis.append({'Unnamed: 0': index,'Score': single_score_dot}, ignore_index=True)\n","\n","        sentiment_analysis[str(index)] = single_score_dot\n","\n","\n","            \n","    return sentiment_analysis"],"metadata":{"id":"sc2wLHu4WA-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Path to the processed tweets \n","csv_path = cwd+\"/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/*.csv\"\n","\n","# List of outcomes\n","outcomes = {}\n","\n","# Loop through all csvs in the folder\n","for file in glob.glob(csv_path):\n","    print(\"Currently at: \" + file)\n","    t1 = time.perf_counter()\n","\n","    # Get the sentiments for the csv file\n","    data = pd.read_csv(file)\n","    results = get_sentiment_sentence(data, word_index)\n","\n","    file_name = str(file)[58:-4]\n","    outcomes[str(file_name)] = results\n","\n","    t2 = time.perf_counter()\n","    print('time taken to run:',t2-t1)\n","    \n","    # Obtain the values and keys from the dictionary returned from get_sentiment function"],"metadata":{"id":"mwrdMBeKWH0Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650829734557,"user_tz":-480,"elapsed":1770483,"user":{"displayName":"Kexin Yeo","userId":"10886170617646031738"}},"outputId":"75dd679e-f214-4e49-b932-0c8b7b65c15a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/ar.csv\n","Model weight loaded successfully\n","time taken to run: 1.6088184630000342\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/bg.csv\n","Model weight loaded successfully\n","time taken to run: 1.0892260339999211\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/ca.csv\n","Model weight loaded successfully\n","time taken to run: 4.1697412000000895\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/cs.csv\n","Model weight loaded successfully\n","time taken to run: 3.922933585999999\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/cy.csv\n","Model weight loaded successfully\n","time taken to run: 0.6890810539998711\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/da.csv\n","Model weight loaded successfully\n","time taken to run: 1.4754093290000583\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/el.csv\n","Model weight loaded successfully\n","time taken to run: 0.98940061899998\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/de.csv\n","Model weight loaded successfully\n","time taken to run: 25.56102605000001\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/en.csv\n","Model weight loaded successfully\n","time taken to run: 1418.690473592\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/es.csv\n","Model weight loaded successfully\n","time taken to run: 105.59127176200036\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/eu.csv\n","Model weight loaded successfully\n","time taken to run: 0.8072357010000815\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/fi.csv\n","Model weight loaded successfully\n","time taken to run: 3.7889741330000106\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/et.csv\n","Model weight loaded successfully\n","time taken to run: 2.173198058000253\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/fr.csv\n","Model weight loaded successfully\n","time taken to run: 31.572598268000093\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/ht.csv\n","Model weight loaded successfully\n","time taken to run: 0.7901436310003191\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/in.csv\n","Model weight loaded successfully\n","time taken to run: 12.418127856999945\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/hi.csv\n","Model weight loaded successfully\n","time taken to run: 1.1311385340000015\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/hu.csv\n","Model weight loaded successfully\n","time taken to run: 0.8908761369998501\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/it.csv\n","Model weight loaded successfully\n","time taken to run: 10.982280891999835\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/ja.csv\n","Model weight loaded successfully\n","time taken to run: 4.033944560999771\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/ko.csv\n","Model weight loaded successfully\n","time taken to run: 0.8176816079999298\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/lt.csv\n","Model weight loaded successfully\n","time taken to run: 0.9122457769999528\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/my.csv\n","Model weight loaded successfully\n","time taken to run: 0.9282066119999399\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/ml.csv\n","Model weight loaded successfully\n","time taken to run: 0.8958829010002773\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/nl.csv\n","Model weight loaded successfully\n","time taken to run: 4.560991165999894\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/no.csv\n","Model weight loaded successfully\n","time taken to run: 1.1174194979998902\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/pl.csv\n","Model weight loaded successfully\n","time taken to run: 7.031686760999946\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/pt.csv\n","Model weight loaded successfully\n","time taken to run: 85.65766335599983\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/ro.csv\n","Model weight loaded successfully\n","time taken to run: 3.1848287880002317\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/ru.csv\n","Model weight loaded successfully\n","time taken to run: 14.221537352000269\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/tl.csv\n","Model weight loaded successfully\n","time taken to run: 2.7108785680002256\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/sl.csv\n","Model weight loaded successfully\n","time taken to run: 1.2741439890000947\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/sv.csv\n","Model weight loaded successfully\n","time taken to run: 3.336044027999833\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/tr.csv\n","Model weight loaded successfully\n","time taken to run: 7.422317269999894\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/uk.csv\n","Model weight loaded successfully\n","time taken to run: 2.9920325939997383\n","Currently at: /content/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/zh.csv\n","Model weight loaded successfully\n","time taken to run: 1.2601400359999388\n"]}]},{"cell_type":"code","source":["path = '/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/output_jan_feb.csv'\n","\n","data_items = outcomes.items()\n","data_list = list(data_items)\n","\n","final_df = pd.DataFrame(data_list)\n","\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  final_df.to_csv(f)"],"metadata":{"id":"QTDch0XSpgPf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jan_feb = pd.read_csv('/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/output_jan_feb.csv')\n","nov_dec = pd.read_csv('/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/output_nov_dec.csv')"],"metadata":{"id":"QrwBbuvl27tZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setting the path for joining multiple files\n","csv_path = cwd+\"/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_jan_feb/*.csv\"\n","\n","# list of merged files returned\n","files = glob.glob(csv_path)\n","\n","# 3. creates empty list to include the content of each file converted to pandas DF\n","csv_list = []\n"," \n","# 4. reads each (sorted) file in file_list, converts it to pandas DF and appends it to the csv_list\n","for file in files:\n","    csv_list.append(pd.read_csv(file).assign(File_Name = os.path.basename(file)))\n","\n","# 5. merges single pandas DFs into a single DF, index is refreshed \n","csv_merged = pd.concat(csv_list, ignore_index=True)\n","\n","csv_merged.loc[(csv_merged['File_Name'] == 'en.csv') & (csv_merged['lang'] == 'und'), 'lang'] = 'en'\n","csv_merged.loc[(csv_merged['File_Name'] == 'un.csv') & (csv_merged['lang'] == 'und'), 'lang'] = 'un'"],"metadata":{"id":"6tzkx3NWcxjY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["column_names = [\"Sentence\", \"sentiment score\", \"sentiment\", \"lang\"]\n","\n","index_df = pd.DataFrame(columns = column_names)\n","index_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"id":"6MnZaX3-aNM2","executionInfo":{"status":"ok","timestamp":1651334205205,"user_tz":-480,"elapsed":29,"user":{"displayName":"Nigel Chua","userId":"17321780526605667435"}},"outputId":"d0d75ff2-4200-46a0-9b3c-eb13ba8e6d75"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [Sentence, sentiment score, sentiment, lang]\n","Index: []"],"text/html":["\n","  <div id=\"df-613cd2e7-af0d-4f0f-8a23-7801f05d41c6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence</th>\n","      <th>sentiment score</th>\n","      <th>sentiment</th>\n","      <th>lang</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-613cd2e7-af0d-4f0f-8a23-7801f05d41c6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-613cd2e7-af0d-4f0f-8a23-7801f05d41c6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-613cd2e7-af0d-4f0f-8a23-7801f05d41c6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["## Jan"],"metadata":{"id":"V6G1QPzIHy8M"}},{"cell_type":"code","source":["import ast\n","from numpy import interp\n","\n","jan_copy = jan_feb.copy()\n","\n","jan_copy.fillna(\"nan\")\n","\n","emtpy_d = {}\n","update = ''\n","\n","for index, row in csv_merged.iterrows():\n","\n","    lang = \"twitter/scrap_jan_feb/\" + row['lang']\n","    \n","    if update != lang or '':\n","\n","        dict_i  = jan_copy[(jan_copy['0'] == lang)]\n","        dict_d = dict_i['1'].iloc[0]\n","        emtpy_d = ast.literal_eval(dict_d)\n","    \n","    key = row['Unnamed: 0']\n","    score = emtpy_d[str(key)]\n","\n","    if score > 0.6:\n","      sen = 'positive'\n","    elif score > 0.4:\n","      sen = 'neutral'\n","    else:\n","      sen = 'negative'\n","\n","    update = lang\n","    new_row = {'Sentence':row['text'], 'sentiment score':score, 'sentiment':sen, 'lang':row['lang']}\n","    index_df = index_df.append(new_row, ignore_index=True)\n","\n","\n","\n","path = '/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/final_jan_feb.csv'\n","\n","final_df = pd.DataFrame(index_df)\n","\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  final_df.to_csv(f)"],"metadata":{"id":"YMSl-T-B6nrf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## nov"],"metadata":{"id":"v6Eze6JwH0RV"}},{"cell_type":"code","source":["# setting the path for joining multiple files\n","csv_path = cwd+\"/gdrive/MyDrive/CS4225/ML team/pre-processed data/twitter/scrap_nov_dec/*.csv\"\n","\n","# list of merged files returned\n","files = glob.glob(csv_path)\n","\n","# 3. creates empty list to include the content of each file converted to pandas DF\n","csv_list = []\n"," \n","# 4. reads each (sorted) file in file_list, converts it to pandas DF and appends it to the csv_list\n","for file in files:\n","    csv_list.append(pd.read_csv(file).assign(File_Name = os.path.basename(file)))\n","\n","# 5. merges single pandas DFs into a single DF, index is refreshed \n","csv_merged = pd.concat(csv_list, ignore_index=True)\n","\n","csv_merged.loc[(csv_merged['File_Name'] == 'en.csv') & (csv_merged['lang'] == 'und'), 'lang'] = 'en'\n","csv_merged.loc[(csv_merged['File_Name'] == 'un.csv') & (csv_merged['lang'] == 'und'), 'lang'] = 'un'\n","\n","\n","column_names = [\"Sentence\", \"sentiment score\", \"sentiment\", \"lang\"]\n","\n","index_df = pd.DataFrame(columns = column_names)\n","index_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"id":"tsQSwZ1AH1D8","executionInfo":{"status":"ok","timestamp":1651334844801,"user_tz":-480,"elapsed":2207,"user":{"displayName":"Nigel Chua","userId":"17321780526605667435"}},"outputId":"0a7d0ee8-ce13-455a-88e0-ca0cb70f4f83"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [Sentence, sentiment score, sentiment, lang]\n","Index: []"],"text/html":["\n","  <div id=\"df-cf345bc2-52bb-4e61-8800-8f266b67835e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence</th>\n","      <th>sentiment score</th>\n","      <th>sentiment</th>\n","      <th>lang</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf345bc2-52bb-4e61-8800-8f266b67835e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cf345bc2-52bb-4e61-8800-8f266b67835e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cf345bc2-52bb-4e61-8800-8f266b67835e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["import ast\n","from numpy import interp\n","\n","nov_copy = nov_dec.copy()\n","\n","nov_copy.fillna(\"nan\")\n","\n","emtpy_d = {}\n","update = ''\n","\n","for index, row in csv_merged.iterrows():\n","\n","    if row['lang'] == 'IS':\n","        continue\n","    \n","    if row['lang'] == 'IT':\n","        continue\n","\n","    if row['lang'] == 'KN':\n","        continue\n","    lang = \"twitter/scrap_nov_dec/\" + row['lang'].lower()\n","    \n","    if update != lang or '':\n","        print(update)\n","        dict_i  = nov_copy[(nov_copy['0'] == lang)]\n","        dict_d = dict_i['1'].iloc[0]\n","        emtpy_d = ast.literal_eval(dict_d)\n","    \n","    key = row['Unnamed: 0']\n","    score = emtpy_d[str(key)]\n","\n","    if score > 0.6:\n","      sen = 'positive'\n","    elif score > 0.4:\n","      sen = 'neutral'\n","    else:\n","      sen = 'negative'\n","\n","    update = lang\n","    new_row = {'Sentence':row['text'], 'sentiment score':score, 'sentiment':sen, 'lang':row['lang']}\n","    index_df = index_df.append(new_row, ignore_index=True)\n","\n","\n","\n","path = '/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/final_nov_dec.csv'\n","\n","final_df = pd.DataFrame(index_df)\n","\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  final_df.to_csv(f)"],"metadata":{"id":"U8ZGnHiS22dl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Only enlgish words"],"metadata":{"id":"5JNHe5dIahE-"}},{"cell_type":"code","source":["jan_feb = pd.read_csv('/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/final_jan_feb.csv')\n","nov_dec = pd.read_csv('/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/final_nov_dec.csv')\n","\n","jan_feb_out = pd.DataFrame(jan_feb)\n","jan_feb_out = jan_feb_out[(jan_feb_out['lang'] == 'en')]\n","\n","nov_dec_out = pd.DataFrame(jan_feb)\n","nov_dec_out = nov_dec_out[(nov_dec_out['lang'] == 'en')]\n"],"metadata":{"id":"LVEKeSoFDsAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","path = '/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/final_nov_dec.csv'\n","\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  nov_dec_out.to_csv(f)"],"metadata":{"id":"bv2EA7V00xlI"},"execution_count":null,"outputs":[]}]}