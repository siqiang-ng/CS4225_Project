{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Prediction_kaggle_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eHmXS-cAVhvz"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import codecs\n","import keras\n","import h5py\n","import jax\n","import jax.numpy as jnp #gonna try use jax since its supposedly faster\n","import time\n","\n","\n","# Keras model, layer and preprocessing libraries\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import LSTM\n","from keras.layers.embeddings import Embedding\n","from keras.layers import Bidirectional\n","from keras.layers import Dropout\n","from keras.models import Sequential\n","from keras.models import model_from_json\n","from keras.models import load_model\n","from keras.preprocessing import sequence\n","\n","# NLTK imports\n","from nltk.tokenize import RegexpTokenizer"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.flush_and_unmount()"],"metadata":{"id":"2P0L5JoMVzfC","executionInfo":{"status":"ok","timestamp":1650852627232,"user_tz":-480,"elapsed":477,"user":{"displayName":"Nigel Chua","userId":"17321780526605667435"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2dea2cf5-c84e-4169-823b-5039c35014fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive not mounted, so nothing to flush and unmount.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount(\"/content/gdrive\")"],"metadata":{"id":"htHRQVrJV0rD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650852649074,"user_tz":-480,"elapsed":21846,"user":{"displayName":"Nigel Chua","userId":"17321780526605667435"}},"outputId":"dc49f1a0-3e83-41af-8f06-a067e00d27cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["import os\n","\n","cwd = os.getcwd()  "],"metadata":{"id":"L8xgTpyWV1yY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Path to the data\n","gloveFile = '/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/glove.6B.100d.txt'\n","\n","word_embedding_dict = {}\n","word_index = {}\n","\n","with codecs.open(gloveFile, 'r', encoding='utf-8') as f:\n","    for line in f:\n","        values = line.split()\n","        # The word that is to be the key\n","        word = values[0]\n","        # Update word_index\n","        word_index[word] = len(word_embedding_dict)\n","        # The vector that represents the word\n","        vector = np.asarray(values[1:], \"float32\")\n","        word_embedding_dict[word] = vector"],"metadata":{"id":"0PAZJV56V3h0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_embedding_matrix = np.zeros((len(word_embedding_dict), 100))\n","\n","i = 0\n","for word in word_embedding_dict.keys():\n","    if i > len(word_embedding_matrix):\n","        break\n","    \n","    word_embedding_vector = word_embedding_dict[word]\n","    if word_embedding_vector is not None:\n","        word_embedding_matrix[i] = word_embedding_vector\n","        i = i + 1"],"metadata":{"id":"FVpgm5PDV-SZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Developing the LSTM Model \n","model = Sequential()\n","# Input_length of 280 is chosen as 280 characters are allowed in twitter\n","model.add(Embedding(len(word_embedding_matrix), 100, weights = [word_embedding_matrix], input_length = 280, trainable = False))\n","model.add(Bidirectional(LSTM(128, dropout = 0.2, recurrent_dropout = 0.2)))\n","model.add(Dense(512, activation = 'relu'))\n","model.add(Dropout(0.50))\n","model.add(Dense(3, activation = 'softmax'))\n","\n","# Adam's optimiser\n","model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","\n","print(model.summary())"],"metadata":{"id":"EVFsWaTOWAdw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650852670002,"user_tz":-480,"elapsed":1367,"user":{"displayName":"Nigel Chua","userId":"17321780526605667435"}},"outputId":"d6e8a3c3-214c-4c16-cb91-57c35a712cbe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 280, 100)          40000000  \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 256)              234496    \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dropout (Dropout)           (None, 512)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 3)                 1539      \n","                                                                 \n","=================================================================\n","Total params: 40,367,619\n","Trainable params: 367,619\n","Non-trainable params: 40,000,000\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["def get_sentiment_sentence(data, word_index):\n","    # Load the RNN model\n","    model.load_weights(\"/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/RNN_Split_Dataset/RNN.h5\")\n","    print(\"Model weight loaded successfully\")\n","    \n","    # Convert keys to lowercase\n","    word_index_lower = {k.lower(): v for k, v in word_index.items()}\n","    \n","    # Preparing the data for prediction\n","    word_list = np.zeros((1, 280), dtype = 'int32')\n","    sentiment_analysis = {}\n","    \n","    for index, indiv_data in data.iterrows():\n","\n","        sentence = indiv_data['text']\n","        #print(indiv_data['text'])\n","        tokenizer = RegexpTokenizer(r'\\w+')\n","        sentence_words = tokenizer.tokenize(str(sentence))\n","           \n","        # Capture all the word into one list for prediction\n","        i = 0\n","        for word in sentence_words:\n","            word_lower = word.lower()\n","            try:\n","                word_list[0][i] = word_index_lower[word_lower]\n","            except Exception as e:\n","                if str(e) == word:\n","                    word_list[0][i] = 0\n","                continue\n","            i = i + 1\n","        \n","        # Predict the score of the tweet \n","        score = model.predict(word_list, batch_size = len(data), verbose = 0)\n","        single_score = np.round(np.argmax(score)/10, decimals=2) # maximum of the array i.e single band\n","        # weighted score of top 3 bands\n","        top_3_index = np.argsort(score)[0][-3:]\n","        top_3_scores = score[0][top_3_index]\n","        top_3_weights = top_3_scores/np.sum(top_3_scores)\n","        single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)\n","        #print(single_score_dot, single_score)\n","        #sentiment_analysis = sentiment_analysis.append({'Unnamed: 0': index,'Score': single_score_dot}, ignore_index=True)\n","\n","        sentiment_analysis[str(index)] = single_score_dot\n","\n","\n","            \n","    return sentiment_analysis"],"metadata":{"id":"sc2wLHu4WA-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","\n","# Path to the processed tweets \n","csv_path = \"/content/gdrive/MyDrive/CS4225/ML team/Sentiment Analysis Results/lexicon/twitter/kaggle_part_1/sentiments/*.csv\"\n","\n","# List of outcomes\n","outcomes = {}\n","\n","# Loop through all csvs in the folder\n","for file in glob.glob(csv_path):\n","    print(\"Currently at: \" + file)\n","    t1 = time.perf_counter()\n","\n","    # Get the sentiments for the csv file\n","    data = pd.read_csv(file)\n","    results = get_sentiment_sentence(data, word_index)\n","\n","    file_name = str(file)[58:-4]\n","    outcomes[str(file_name)] = results\n","\n","    t2 = time.perf_counter()\n","    print('time taken to run:',t2-t1)\n","    \n","    # Obtain the values and keys from the dictionary returned from get_sentiment function"],"metadata":{"id":"mwrdMBeKWH0Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = '/content/gdrive/MyDrive/CS4225/ML team/pre-processed data/stanfordSentimentTreebank/output_kaggle_1.csv'\n","\n","data_items = outcomes.items()\n","data_list = list(data_items)\n","\n","final_df = pd.DataFrame(data_list)\n","\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  final_df.to_csv(f)"],"metadata":{"id":"QTDch0XSpgPf"},"execution_count":null,"outputs":[]}]}